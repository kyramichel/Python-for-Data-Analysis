{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U7TRodEP4sV"
      },
      "source": [
        "# Logistic Regression for Anti-Money Laundering (AML)\n",
        "\n",
        "## Handling Duplicates and Assessing Degrees of Freedom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwLWdFvUVFFz"
      },
      "source": [
        "\n",
        "In this notebook, we created a synthetic dataset to test fraud detection algorithms for Anti-Money Laundering (AML). We highlighted the importance of including duplicates in the dataset, as they help the model learn to identify potential fraud.\n",
        "\n",
        "We performed data cleaning and preprocessing to prepare the dataset for analysis. Then, we built a logistic regression model to classify transactions as legitimate or fraudulent using features like `UserID` and `Amount`.\n",
        "\n",
        "We also discussed the role of degrees of freedom in understanding model complexity and evaluating performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPtSGjdTP5qv"
      },
      "source": [
        "## Part 1: Dataset Creation\n",
        "\n",
        "The code below generates a synthetic dataset that simulates real-world scenarios for testing fraud detection algorithms, particularly for identifying duplicate financial transactions and salary or claim records.\n",
        "\n",
        "The dataset consists of a total of 1,300 records, structured as follows:\n",
        "- **Number of Unique Transaction Records**: 1,000\n",
        "- **Number of Duplicate Records**: 300 (introduced to simulate fraud or entry errors)\n",
        "\n",
        "Duplicates are introduced by randomly selecting existing records and creating new entries with slight variations:\n",
        "- Each duplicate is assigned a new **TransactionID**.\n",
        "- The **TransactionDate** is slightly modified by adding a random number of minutes (ranging from 1 to 60).\n",
        "\n",
        "After generating the records, the dataset is shuffled to randomize the order of entries.\n",
        "\n",
        "A new column, **IsDuplicate**, is added to indicate whether a record is a duplicate based on the combination of **UserID** and **Amount**. This column is set to 1 for duplicates and 0 for unique records.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4zQvWX3P59S",
        "outputId": "b7ae9b7e-0801-4abe-b2f4-441e7c20e1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic dataset created and saved to 'synthetic_transactions.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters\n",
        "num_records = 1000 # Number of unique records\n",
        "num_duplicates = 300  # Number of duplicates to introduce\n",
        "\n",
        "# Generate unique records\n",
        "user_ids = np.random.randint(1, 100, size=num_records)\n",
        "amounts = np.random.uniform(100, 10000, size=num_records)\n",
        "transaction_dates = [datetime.now() - timedelta(days=np.random.randint(0, 30)) for _ in range(num_records)]\n",
        "transaction_types = np.random.choice(['salary', 'claim', 'purchase'], size=num_records)\n",
        "locations = np.random.choice(['Location_A', 'Location_B', 'Location_C'], size=num_records)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'TransactionID': range(1, num_records + 1),\n",
        "    'UserID': user_ids,\n",
        "    'Amount': amounts,\n",
        "    'TransactionDate': transaction_dates,\n",
        "    'TransactionType': transaction_types,\n",
        "    'Location': locations,\n",
        "    'Status': 'completed'\n",
        "})\n",
        "\n",
        "# Introduce duplicates\n",
        "for i in range(num_duplicates):\n",
        "    # Randomly select a record to duplicate\n",
        "    duplicate_record = data.sample(1).copy()\n",
        "    duplicate_record['TransactionID'] = num_records + i + 1  # New ID for duplicate\n",
        "    # Ensure the duplicate has the same UserID and Amount\n",
        "    duplicate_record['TransactionDate'] += timedelta(minutes=np.random.randint(1, 60))  # Slightly alter date\n",
        "    data = pd.concat([data, duplicate_record], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Labeling duplicates based on UserID and Amount (this is appropriate for the use case)\n",
        "data['IsDuplicate'] = data.duplicated(subset=['UserID', 'Amount'], keep=False).astype(int)\n",
        "\n",
        "# Save to CSV\n",
        "data.to_csv('synthetic_transactions.csv', index=False)\n",
        "\n",
        "print(\"Synthetic dataset created and saved to 'synthetic_transactions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "JF86PdnZdKNt",
        "outputId": "9a5644bd-d048-4886-bc6b-27356a0ac513"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1300,\n  \"fields\": [\n    {\n      \"column\": \"TransactionID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 375,\n        \"min\": 1,\n        \"max\": 1300,\n        \"num_unique_values\": 1300,\n        \"samples\": [\n          64,\n          714,\n          582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UserID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 1,\n        \"max\": 99,\n        \"num_unique_values\": 99,\n        \"samples\": [\n          67,\n          3,\n          98\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2847.1328204854617,\n        \"min\": 102.35148340125734,\n        \"max\": 9993.599645701765,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          6936.546301211842,\n          2422.156804014804,\n          6369.0836716185095\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TransactionDate\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1299,\n        \"samples\": [\n          \"2025-05-11 01:47:02.662485\",\n          \"2025-05-16 00:57:02.666182\",\n          \"2025-06-03 00:57:02.665301\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TransactionType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"salary\",\n          \"purchase\",\n          \"claim\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Location_B\",\n          \"Location_C\",\n          \"Location_A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"completed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IsDuplicate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c39ec109-b90e-4bd8-94e7-a9681b5f4453\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>UserID</th>\n",
              "      <th>Amount</th>\n",
              "      <th>TransactionDate</th>\n",
              "      <th>TransactionType</th>\n",
              "      <th>Location</th>\n",
              "      <th>Status</th>\n",
              "      <th>IsDuplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>262</td>\n",
              "      <td>90</td>\n",
              "      <td>1230.955467</td>\n",
              "      <td>2025-06-01 00:57:02.661825</td>\n",
              "      <td>salary</td>\n",
              "      <td>Location_B</td>\n",
              "      <td>completed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1142</td>\n",
              "      <td>35</td>\n",
              "      <td>1111.905376</td>\n",
              "      <td>2025-05-29 01:52:02.660998</td>\n",
              "      <td>purchase</td>\n",
              "      <td>Location_C</td>\n",
              "      <td>completed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c39ec109-b90e-4bd8-94e7-a9681b5f4453')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c39ec109-b90e-4bd8-94e7-a9681b5f4453 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c39ec109-b90e-4bd8-94e7-a9681b5f4453');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c76ce85b-931c-496d-99f4-dfb4ed17ce72\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c76ce85b-931c-496d-99f4-dfb4ed17ce72')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c76ce85b-931c-496d-99f4-dfb4ed17ce72 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   TransactionID  UserID       Amount             TransactionDate  \\\n",
              "0            262      90  1230.955467  2025-06-01 00:57:02.661825   \n",
              "1           1142      35  1111.905376  2025-05-29 01:52:02.660998   \n",
              "\n",
              "  TransactionType    Location     Status  IsDuplicate  \n",
              "0          salary  Location_B  completed            1  \n",
              "1        purchase  Location_C  completed            1  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHL_NGOof-OH",
        "outputId": "f45961d7-b9c1-4acc-e618-7ee517c83354"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1300"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi1gbwCCdc8P",
        "outputId": "da0ae3f0-9790-49d6-b9e8-27bf31e788fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "# Get the number of columns in the dataset\n",
        "num_columns = len(data.columns)\n",
        "print(num_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wx-nnOjcUMB",
        "outputId": "ebb59d56-024d-427b-e2ce-a0eedf9b082b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       TransactionID       UserID       Amount                TransactionDate  \\\n",
            "count    1300.000000  1300.000000  1300.000000                           1300   \n",
            "mean      650.500000    49.546154  5120.406928  2025-05-19 06:46:24.638590720   \n",
            "min         1.000000     1.000000   102.351483     2025-05-05 01:50:17.990577   \n",
            "25%       325.750000    24.000000  2641.001010  2025-05-12 01:50:17.991203328   \n",
            "50%       650.500000    49.500000  5219.979966  2025-05-19 01:50:17.992592384   \n",
            "75%       975.250000    74.250000  7591.742530  2025-05-27 01:50:17.993413632   \n",
            "max      1300.000000    99.000000  9993.599646     2025-06-03 03:36:17.993745   \n",
            "std       375.421985    29.112960  2847.132820                            NaN   \n",
            "\n",
            "       IsDuplicate  \n",
            "count  1300.000000  \n",
            "mean      0.403077  \n",
            "min       0.000000  \n",
            "25%       0.000000  \n",
            "50%       0.000000  \n",
            "75%       1.000000  \n",
            "max       1.000000  \n",
            "std       0.490705  \n"
          ]
        }
      ],
      "source": [
        "# Summary statistics for numerical features\n",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLGKh-e6gCVU"
      },
      "source": [
        "### Duplicates:\n",
        "\n",
        "Including duplicates in the dataset is essential for training models aimed at identifying fraudulent transactions. The presence of duplicates allows the model to learn the characteristics of transactions that are likely to be flagged as duplicates or fraudulent.\n",
        "\n",
        "The duplicates created in this dataset are not exact matches across all columns, as the `TransactionDate` has been modified. However, we label duplicates based on `UserID` and `Amount`, which is appropriate for this use case. By clearly labeling duplicates, we provide the model with explicit examples of what constitutes a duplicate transaction, which is crucial for effective training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcIc3JV5QXs3"
      },
      "source": [
        "\n",
        "## Generic Data Cleaning\n",
        "\n",
        "The data cleaning code below addresses several key aspects, including handling missing values, removing any unintended duplicates, and ensuring that the data types are appropriate for analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9dXRVfVQdNj",
        "outputId": "6633337d-6f48-4589-efe3-587f1f16880e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values:\n",
            "TransactionID      0\n",
            "UserID             0\n",
            "Amount             0\n",
            "TransactionDate    0\n",
            "TransactionType    0\n",
            "Location           0\n",
            "Status             0\n",
            "IsDuplicate        0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Records Before Removal:\n",
            "0\n",
            "\n",
            "Data Types After Cleaning:\n",
            "TransactionID               int64\n",
            "UserID                      int64\n",
            "Amount                    float64\n",
            "TransactionDate    datetime64[ns]\n",
            "TransactionType            object\n",
            "Location                   object\n",
            "Status                     object\n",
            "IsDuplicate                 int64\n",
            "dtype: object\n",
            "\n",
            "Cleaned Data:\n",
            "   TransactionID  UserID       Amount            TransactionDate  \\\n",
            "0            262      90  1230.955467 2025-06-01 01:50:17.991587   \n",
            "1           1142      35  1111.905376 2025-05-29 02:45:17.991105   \n",
            "2            463      55   539.328969 2025-05-12 01:50:17.992295   \n",
            "\n",
            "  TransactionType    Location     Status  IsDuplicate  \n",
            "0          salary  Location_B  completed            1  \n",
            "1        purchase  Location_C  completed            1  \n",
            "2        purchase  Location_A  completed            0  \n",
            "\n",
            "Cleaned dataset saved to 'cleaned_synthetic_transactions.csv'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-e135a0208c48>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Amount'].fillna(data['Amount'].mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('synthetic_transactions.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# Fill missing values with appropriate methods\n",
        "# Here, I'll fill missing 'Amount' with the mean, and drop any rows with missing 'UserID' or 'TransactionDate'\n",
        "data['Amount'].fillna(data['Amount'].mean(), inplace=True)\n",
        "data.dropna(subset=['UserID', 'TransactionDate'], inplace=True)\n",
        "\n",
        "# Check for duplicate records\n",
        "print(\"\\nDuplicate Records Before Removal:\")\n",
        "print(data.duplicated().sum())\n",
        "\n",
        "# Remove Duplicates using drop_duplicates method\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "# Convert 'TransactionDate' to datetime\n",
        "data['TransactionDate'] = pd.to_datetime(data['TransactionDate'])\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData Types After Cleaning:\")\n",
        "print(data.dtypes)\n",
        "\n",
        "# Reset index after cleaning\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the cleaned dataset\n",
        "print(\"\\nCleaned Data:\")\n",
        "print(data.head(3))\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "data.to_csv('cleaned_synthetic_transactions.csv', index=False)\n",
        "print(\"\\nCleaned dataset saved to 'cleaned_synthetic_transactions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kKy2Y5pSNoq"
      },
      "source": [
        "\n",
        "## Data Cleaning in the Context of Building a Logistic Regression Model for AML Detection\n",
        "\n",
        "### Understanding Duplicates in AML Modeling\n",
        "\n",
        "**Purpose of Duplicates**:\n",
        "- Duplicates in the dataset can signify potential fraud or errors in transaction entries. In the context of Anti-Money Laundering (AML), these duplicates often represent the very cases that need to be identified and flagged.\n",
        "- Removing duplicates may inadvertently eliminate important examples of fraudulent behavior that the model needs to learn from.\n",
        "\n",
        "**Using Raw Data with Duplicates**:\n",
        "- When training your model, it is generally advisable to include the raw data with duplicates. This allows the model to learn the patterns associated with both legitimate and potentially fraudulent transactions.\n",
        "- The presence of duplicates can help the model understand the characteristics of transactions that are likely to be flagged as duplicates or fraudulent.\n",
        "\n",
        "**Labeling Duplicates**:\n",
        "- Instead of removing duplicates, we can label them appropriately. For instance, we created a binary target variable (e.g., `IsDuplicate`) that indicates whether a transaction is a duplicate.\n",
        "- This approach enables the model to differentiate between legitimate transactions and those that are likely to be fraudulent or erroneous.\n",
        "\n",
        "### Recommended Approach:\n",
        "\n",
        "**Keep Duplicates**: Use the dataset with duplicates included for training your Logistic Regression model. This will provide the model with a more comprehensive view of the data.\n",
        "\n",
        "**Labeling**: Ensure that you have a clear labeling strategy for your target variable. For example, label duplicates as `1` (fraudulent) and unique transactions as `0` (legitimate).\n",
        "\n",
        "### Part 2: Updated Data Cleaning:\n",
        "\n",
        "While it is important to keep duplicates for training, we may still want to perform other cleaning steps, such as handling missing values, correcting data types, and normalizing or scaling numerical features:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq7ATODbiWIP",
        "outputId": "ad94ef86-11b4-49dc-f660-1f7c693e3ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Data:\n",
            "   TransactionID  UserID       Amount             TransactionDate  \\\n",
            "0            262      90  1230.955467  2025-06-01 01:50:17.991587   \n",
            "1           1142      35  1111.905376  2025-05-29 02:45:17.991105   \n",
            "\n",
            "  TransactionType    Location     Status  IsDuplicate  \n",
            "0          salary  Location_B  completed            1  \n",
            "1        purchase  Location_C  completed            1  \n",
            "\n",
            "Missing Values:\n",
            "TransactionID      0\n",
            "UserID             0\n",
            "Amount             0\n",
            "TransactionDate    0\n",
            "TransactionType    0\n",
            "Location           0\n",
            "Status             0\n",
            "IsDuplicate        0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Records Before Removal:\n",
            "524\n",
            "\n",
            "Data Types After Cleaning:\n",
            "TransactionID               int64\n",
            "UserID                      int64\n",
            "Amount                    float64\n",
            "TransactionDate    datetime64[ns]\n",
            "TransactionType            object\n",
            "Location                   object\n",
            "Status                     object\n",
            "IsDuplicate                 int64\n",
            "dtype: object\n",
            "\n",
            "New dataset for modeling saved to 'data_for_modeling.csv'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-30b3452ca228>:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Amount'].fillna(data['Amount'].mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('synthetic_transactions.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Initial Data:\")\n",
        "print(data.head(2))\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# Fill missing values with appropriate methods\n",
        "# Here, I'll fill missing 'Amount' with the mean, and drop any rows with missing 'UserID' or 'TransactionDate'\n",
        "data['Amount'].fillna(data['Amount'].mean(), inplace=True)\n",
        "data.dropna(subset=['UserID', 'TransactionDate'], inplace=True)\n",
        "\n",
        "# Check for duplicate records\n",
        "print(\"\\nDuplicate Records Before Removal:\")\n",
        "print(data.duplicated(subset=['UserID', 'Amount'], keep=False).sum())\n",
        "\n",
        "# Create a new dataset for modeling, keeping duplicates\n",
        "data_for_modeling = data.copy()\n",
        "\n",
        "# Convert 'TransactionDate' to datetime\n",
        "data_for_modeling['TransactionDate'] = pd.to_datetime(data_for_modeling['TransactionDate'])\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData Types After Cleaning:\")\n",
        "print(data_for_modeling.dtypes)\n",
        "\n",
        "# Reset index after cleaning\n",
        "data_for_modeling.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Create a duplicate label\n",
        "data_for_modeling['IsDuplicate'] = data_for_modeling.duplicated(subset=['UserID', 'Amount'], keep=False).astype(int)\n",
        "\n",
        "# Save the new dataset for modeling to a new CSV file\n",
        "data_for_modeling.to_csv('data_for_modeling.csv', index=False)\n",
        "print(\"\\nNew dataset for modeling saved to 'data_for_modeling.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FukgxUDbnjQJ"
      },
      "source": [
        "### Key points on updated data cleaning code:\n",
        "\n",
        "- It loads the synthetic dataset and checks for missing values.\n",
        "- It handles missing values by filling the `Amount` column with the mean and dropping rows with missing `UserID` or `TransactionDate`.\n",
        "- It checks for duplicate records and creates a new dataset for modeling while retaining duplicates.\n",
        "- The `TransactionDate` is converted to a datetime format, and the data types are displayed after cleaning.\n",
        "- Finally, the cleaned dataset is saved to a new CSV file for further modeling:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K42d8B39oje5"
      },
      "source": [
        "\n",
        "## Part 3: Building a Logistic Regression Model for AML\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Load the Updated Cleaned Dataset**: Import the dataset prepared for modeling.\n",
        "2. **Prepare Features and Target**: Define the features (independent variables) and the target (dependent variable).\n",
        "3. **Split the Dataset**: Divide the dataset into training and testing sets to evaluate model performance.\n",
        "4. **Fit the Logistic Regression Model**: Train the model using the training set.\n",
        "5. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
        "6. **Evaluate the Model**: Assess the model's performance using metrics such as precision, recall, and F1-score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoNPXplNiXPp"
      },
      "source": [
        "\n",
        "### Data Preprocessing and Checking the Distribution of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKmpP-49SN0G",
        "outputId": "6a2476a7-d4ad-4468-8730-475ecdab3b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Variable Distribution:\n",
            "IsDuplicate\n",
            "0    776\n",
            "1    524\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the cleaned dataset\n",
        "data = pd.read_csv('data_for_modeling.csv')\n",
        "\n",
        "# Prepare features and target\n",
        "X = data[['UserID', 'Amount']]\n",
        "y = data['IsDuplicate']\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Ensure there are samples of both classes\n",
        "if y.value_counts().min() < 1:\n",
        "    print(\"Not enough samples of both classes to train the model.\")\n",
        "else:\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkb7GS70oJVT"
      },
      "source": [
        "\n",
        "### Key Points:\n",
        "- The code loads the cleaned dataset and prepares the features (`X`) and target variable (`y`).\n",
        "- It checks the distribution of the target variable to ensure that there are samples of both classes (duplicates and non-duplicates).\n",
        "- If there are insufficient samples of either class, a warning message is printed.\n",
        "- If the distribution is adequate, the dataset is split into training and testing sets using a 70-30 split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD0s_PM_Q7_s",
        "outputId": "6e272596-e2e3-4965-a608-99982b45e6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75       234\n",
            "           1       0.00      0.00      0.00       156\n",
            "\n",
            "    accuracy                           0.60       390\n",
            "   macro avg       0.30      0.50      0.38       390\n",
            "weighted avg       0.36      0.60      0.45       390\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[234   0]\n",
            " [156   0]]\n",
            "Degrees of Freedom: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Fit Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate degrees of freedom\n",
        "num_features = X.shape[1]\n",
        "degrees_of_freedom = num_features + 1  # Including intercept\n",
        "print(f'Degrees of Freedom: {degrees_of_freedom}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKMaHatpTat5"
      },
      "source": [
        "\n",
        "### Interpretation of Degrees of Freedom\n",
        "\n",
        "In this logistic regression model, there are 2 features: `UserID` and `Amount`. The degrees of freedom is 3, which accounts for the 2 features plus 1 intercept. This value is important for understanding model complexity and conducting statistical tests:\n",
        "\n",
        "- **Model Complexity**: Degrees of freedom indicate the model's complexity. A higher number means more parameters, allowing for a better fit to the training data but increasing the risk of overfitting.\n",
        "\n",
        "- **Statistical Testing**: Degrees of freedom are used in hypothesis testing to determine critical values from statistical distributions, such as the Chi-squared distribution. They help assess the significance of model parameters, especially in likelihood ratio tests comparing nested models.\n",
        "\n",
        "- **Model Evaluation**: Degrees of freedom can be used with metrics like AIC or BIC to evaluate model fit. Models with fewer parameters are generally preferred if they provide a similar fit, as they are less likely to overfit.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
