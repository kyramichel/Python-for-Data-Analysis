{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4076d32",
   "metadata": {},
   "source": [
    "\n",
    "This notebook will briefly cover three common data preprocessing methods in Python, which are essential for effective data cleaning:\n",
    "\n",
    "1. **Selecting Unique Elements**: Using Python's `set()` function to extract distinct values from datasets, helping to eliminate duplicates.\n",
    "\n",
    "2. **Degrees of Freedom in Logistic Regression**: Understanding how to calculate degrees of freedom, which is important for evaluating model complexity.\n",
    "\n",
    "3. **Multicollinearity**: Detecting multicollinearity among independent variables using the Variance Inflation Factor (VIF), which helps assess the reliability of regression coefficients.\n",
    "\n",
    "Additionally, **Dropping Unnecessary Columns**: The `drop()` method can remove irrelevant or redundant features from the dataset, streamlining analysis. Similarly, SQL's `DISTINCT` keyword can filter out duplicate records in a database. SQL is also effective for data preprocessing tasks like filtering, aggregating, and joining datasets, making it a valuable tool alongside Python for data manipulation. Together, these methods enhance the data cleaning and preparation process, leading to more accurate analyses and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74140cab",
   "metadata": {},
   "source": [
    "### 1. Selecting Unique Elements\n",
    "\n",
    "In Python, you can use the `set()` function to select unique elements from a list or any iterable. This is particularly useful when processing datasets to ensure that you are working with distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bcbc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: a list of credit scores\n",
    "credit_scores = [700, 720, 700, 680, 720, 750, 680]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8486e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Credit Scores: {720, 700, 680, 750}\n"
     ]
    }
   ],
   "source": [
    "# Using set() to get unique credit scores\n",
    "unique_credit_scores = set(credit_scores)\n",
    "print(\"Unique Credit Scores:\", unique_credit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e383195",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Degrees of Freedom in Logistic Regression\n",
    "\n",
    "Degrees of freedom can be calculated as $ n - p - 1 $, where $n$ \\)$ is the number of observations and $p $ is the number of parameters estimated in the model. Degrees of freedom can help evaluating the model's complexity -  a higher number of degrees of freedom generally indicates a more complex model, while a lower number may suggest overfitting or insufficient data to support the model's parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8c75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.536857\n",
      "         Iterations 6\n",
      "\n",
      "Model Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                   60\n",
      "Model:                          Logit   Df Residuals:                       58\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 30 May 2025   Pseudo R-squ.:                  0.1212\n",
      "Time:                        22:23:01   Log-Likelihood:                -32.211\n",
      "converged:                       True   LL-Null:                       -36.652\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.002882\n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           21.4682      8.549      2.511      0.012       4.713      38.223\n",
      "credit_score    -0.0315      0.012     -2.592      0.010      -0.055      -0.008\n",
      "================================================================================\n",
      "\n",
      "Degrees of Freedom: 58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Expanded sample dataset with more observations\n",
    "data = {\n",
    "    'credit_score': [\n",
    "        700, 720, 680, 750, 690, 710, 740, 730, 675, 665,\n",
    "        780, 800, 690, 720, 710, 680, 740, 760, 690, 700,\n",
    "        720, 710, 680, 750, 690, 720, 740, 730, 675, 685,\n",
    "        700, 720, 680, 750, 690, 710, 740, 730, 675, 665,\n",
    "        780, 800, 690, 720, 710, 680, 740, 760, 690, 700,\n",
    "        720, 710, 680, 750, 690, 720, 740, 730, 675, 685\n",
    "    ],\n",
    "    'default': [\n",
    "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
    "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adding a constant for the intercept\n",
    "X = sm.add_constant(df['credit_score'])\n",
    "y = df['default']\n",
    "\n",
    "# Fitting the logistic regression model\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Degrees of freedom\n",
    "n_observations = len(df)\n",
    "n_parameters = model.params.shape[0]  # Includes intercept\n",
    "degrees_of_freedom = n_observations - n_parameters\n",
    "\n",
    "print(\"\\nDegrees of Freedom:\", degrees_of_freedom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56368b3d",
   "metadata": {},
   "source": [
    "## 3. Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable estimates of coefficients. You can detect multicollinearity using the Variance Inflation Factor (VIF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c28d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factor (VIF):\n",
      "        feature          VIF\n",
      "0  credit_score   238.603998\n",
      "1        income  2108.113632\n",
      "2          debt  1242.875791\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Sample dataset with multiple features\n",
    "data_multicollinearity = {\n",
    "    'credit_score': [700, 720, 680, 750, 690, 710, 740],\n",
    "    'income': [50000, 60000, 55000, 70000, 52000, 58000, 62000],\n",
    "    'debt': [20000, 25000, 22000, 30000, 21000, 23000, 24000]\n",
    "}\n",
    "\n",
    "df_multicollinearity = pd.DataFrame(data_multicollinearity)\n",
    "\n",
    "# Calculating VIF for each feature\n",
    "X_multicollinearity = df_multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_multicollinearity.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_multicollinearity.values, i) for i in range(X_multicollinearity.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893d2a8",
   "metadata": {},
   "source": [
    "## 4. Correlation Matrix\n",
    "\n",
    "By using the `corr()` method, you can generate a **correlation matrix** that displays the correlation coefficients between pairs of variables. A high correlation coefficient (close to +1 or -1) between two independent variables suggests that they may be multicollinear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdc8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "      var1  var2  var3  var4\n",
      "var1   1.0   1.0  -1.0   NaN\n",
      "var2   1.0   1.0  -1.0   NaN\n",
      "var3  -1.0  -1.0   1.0   NaN\n",
      "var4   NaN   NaN   NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'var1': [1, 2, 3, 4, 5],\n",
    "    'var2': [2, 4, 6, 8, 10],  # Highly correlated with var1\n",
    "    'var3': [5, 4, 3, 2, 1],   # Inversely correlated with var1\n",
    "    'var4': [1, 1, 1, 1, 1]    # Constant variable\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af4c4b",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "\n",
    "- **Unique Elements**: We utilized the `set()` function to extract unique credit scores from a list, ensuring that we eliminate any duplicates in our dataset.\n",
    "- **Degrees of Freedom**: We calculated the degrees of freedom in a logistic regression model by subtracting the number of estimated parameters from the total number of observations, which is essential for evaluating model complexity.\n",
    "- **Multicollinearity**: We assessed multicollinearity among independent variables using the Variance Inflation Factor (VIF), which quantifies how much the variance of a regression coefficient is inflated due to correlations between predictors.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
